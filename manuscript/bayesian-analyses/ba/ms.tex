\RequirePackage{etoolbox}
\csdef{input@path}{%
 {sty/}% cls, sty files
 {img/}% eps files
}%
\csgdef{bibdir}{bib/}% bst, bib files

\documentclass[ba]{imsart}
%
\pubyear{0000}
\volume{00}
\issue{0}
\doi{0000}
\firstpage{1}
\lastpage{1}

%
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{natbib}
\usepackage[colorlinks,citecolor=blue,urlcolor=blue,filecolor=blue,backref=page]{hyperref}
\usepackage{graphicx}

\startlocaldefs
\numberwithin{equation}{section}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\endlocaldefs

\usepackage{color}
\newcommand{\falta}[1]{\textcolor{red}{#1}}

\begin{document}

\begin{frontmatter}
\title{A generalized Dirichlet distribution with scaled variances}
\runtitle{Generalized Dirichlet}
%%\thankstext{T1}{Footnote to the title with the ``thankstext'' command.}

\begin{aug}
\author{\fnms{Bret}
  \snm{Larget}\thanksref{addr1}\ead[label=e1]{bret.larget@wisc.edu}}
\and
\author{\fnms{Claudia} \snm{Sol\'{i}s-Lemus}\thanksref{addr1}\ead[label=e2]{solislemus@wisc.edu}}


\runauthor{B. Larget and C. Sol\'{i}s-Lemus}

\address[addr1]{Department of Botany
  132 Birge Hall, 
  430 Lincoln Drive, 
  Madison, WI 53706
    \printead{e1} % print email address of "e1"
    \printead*{e2}
}

\end{aug}

\begin{abstract}
\falta{xxx}
\end{abstract}

\begin{keyword}[class=MSC]
\kwd[Primary ]{\falta{60K35}}
\kwd{\falta{60K35}}
\kwd[; secondary ]{\falta{60K35}}
\end{keyword}

\begin{keyword}
\kwd{\falta{xxx}}
\kwd{\falta{xxx}}
\end{keyword}

\end{frontmatter}

\section{The Dirichlet distribution}

The Dirichlet distribution is a very commonly used probability
distribution on sets of positive random variables constrained to sum
to one.  The random variables $X_1,\ldots,X_k$ are said to have a
Dirichlet distribution when they have the joint density
$$
f(x_1,\ldots,x_k) = \frac{\Gamma(\alpha_1 + \cdots +
  \alpha_k)}{\prod_{i=1}^k \Gamma(\alpha_i)} \prod_{i=1}^k
x_i^{\alpha_i - 1}, \qquad \text{where $x_i > 0$ for all $i$ and
  $\sum_{i=1}^k x_i = 1$}
$$
where the parameters $\alpha_i > 0$ for $i=1,\ldots,k$.

Each random variable $X_i$ has a marginal
$\text{Beta}(\alpha_i,\theta-\alpha_i)$ distribution where $\theta =
\sum_{i=1}^k \alpha_i$.  It follows that $X_i$ has mean
$\mathsf{E}(X_i) = \alpha_i/\theta$ and variance $\mathsf{Var}(X_i) =
\alpha_i(\theta-\alpha_i) / ( \theta^2(\theta+1) )$.  A consequence is
that when attempting to select a Dirichlet distribution to match the
distribution of a given set of random variables constrained to equal
one, while it is possible to select the parameters $\{\alpha_i\}$ to
match the marginal means by letting $\alpha_i$ be proportional to the
desired marginal mean, there remains only a single scale factor which
determines all of the marginal variances.  We seek a generalization
with variation of scale factors in the variances, and with a larger
parameterization that is flexible enough to match, at least
approximately, the means and variances of each marginal distribution.

We know of another generalization of the Dirichlet distribution
(described \falta{HERE}) that is different than what we propose here
in that it is asymmetric in the indices of the random variables and
has the property that some correlations may be positive.

\subsection{Generation of random variables}

To generate random variables $X_1,\ldots,X_k \sim
\text{Dirichlet}(\alpha_1,\ldots,\alpha_k)$, one simply generates
independent random variables $Y_i \sim \text{Gamma}(\alpha_i,\lambda)$
for $i=1,\ldots,k$ and any arbitrary $\lambda>0$ (typically
$\lambda=1$) and letting $X_i = Y_i / \sum_{j=1}^k Y_j$.  This
suggests that by allowing the value of $\lambda$ to vary with $i$ that
we may be able to create a distribution on positive random variables
constrained to sum to one with the desired flexibility in the first
and second moments.

\section{A Generalized Dirichlet Distribution}

Define the symmetric generalized Dirichlet distribution on
$X_1,\ldots,X_n$ to be the distribution of $(X_1,\ldots,X_k)$ where
$X_i = Y_i \big/ \sum_{j=1}^k Y_j$ for $i=1,\ldots,k$ where the random
variables $\{Y_i\}$ are mutually independent and $Y_i \sim
\text{Gamma}(\alpha_i,\lambda_i)$ (see also \falta{Craiu and Craiu}).  As the distribution of the
$\{X_i\}$ would be the same if all $\{Y_i\}$ were multiplied by a
common constant, we add the constraint that $\sum_{i=1}^k \lambda_i =
k$ so that the average values of the $\{\lambda_i\}$ parameters is
one.  \falta{(CHECK IF SETTING MEAN OF $1/\lambda_i$ TO BE ONE IS ANY MORE
CONVENIENT).}

It is known \falta{(REFERENCES)} that the distribution of the sum $S =
\sum_{i=1}^k Y_i$ may be written as an infinite mixture of Gamma
densities.  However, the joint density of $X_1,\ldots,X_k)$ has a
closed form solution.
$$
f(x_1,\ldots,x_k) = \frac{\Gamma\big(\sum_{i=1}^k
  \alpha_i\big)\big(\prod_{i=1}^k
  \lambda_i^{\alpha_i}\big)}{\prod_{i=1}^k \Gamma(\alpha_i)} \times
\frac{\prod_{i=1}^k x_i^{\alpha_i - 1}}{\big(\sum_{i=1}^k \lambda_i
  x_i\big)^{\sum_{i=1}^k \alpha_i}}, \qquad \text{where $x_i > 0$ for
  all $i$ and $\sum_{i=1}^k x_i = 1$}
$$
\falta{The derivation is shown in the appendix.}

I have not been able to derive closed form solutions for the marginal
means and variances, but the means are close (if not exactly equal to)
$(\alpha_i/\lambda_i) \big/ \sum_{j=1}^k (\alpha_j/\lambda_j)$.

\subsection{Parameter Estimation}

Suppose that a probability density $g$ on the $k$-dimensional simplex
has marginal mean $\{\mu_i\}$ and marginal variances $\{v_i\}$.  We do
the following.
$$
\alpha_i = \frac{\mu_i^2(1-\mu_i)}{v_i}
$$
$$
\lambda_i = \frac{\mu_i(1-\mu_i)}{v_i} \Bigg/ \sum_{j=1}^k
\frac{\mu_j(1-\mu_j)}{kv_j}
$$
By construction, the mean of the $\{\lambda_i\}$ is one.

\falta{I need to provide some more theoretical evidence that these
  parameter estimates work.}

\section{Simulations}

\falta{simulate generalized dirichlet, pick alpha and lambda based on
  mean/variance, show that it is a good fit}

% Example of a theorem:

% \begin{thm}
% All conjectures are interesting, but some conjectures are more
% interesting than others.
% \end{thm}

% \begin{proof}
% Obvious.
% \end{proof}


% \begin{supplement}
% \sname{Supplement A}\label{suppA} 
% \stitle{Title of the Supplement A}
% \slink[url]{http://www.some-url-address.org/dowload/0000.zip}
% \sdescription{Add description for supplement material.}
% \end{supplement}

\falta{To do:}
\begin{itemize}
\item \falta{justify moments} 
\item \falta{what are marginals?} 
\item \falta{does this work for small alpha (only tested with big
    alpha)?}
\item \falta{we need to show that the mean/variance exist, or give
    boundaries. also, marginal?}
\item \falta{there are possible constraints for what the variances can
    be given a set of means}
\item \falta{even if we do not have expressions for moments, can we
    show that the density is unimodal and is there an expression for
    the marginal mode?}
\end{itemize}


\bibliographystyle{ba}
\bibliography{sample}

\begin{acknowledgement}
\falta{xxx}
\end{acknowledgement}


\end{document}

